{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UE2 - Feature Engineering und Klassifikation mit sklearn\n",
    "Author: Bleyel Andreas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2d Kaggle Challenge\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, median_absolute_error\n",
    "from sklearn.model_selection import cross_val_score, KFold, RandomizedSearchCV\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspektion des Datensets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu Beginn ist gleich einmal zu sehen, dass wir es mit sehr vielen Feature-Variablen zu tun haben. Eine Reduktion auf solche welche einen signifikanten Einfluss auf die Vorhersage der Ziel-Variable \"Sales Price\" haben, hat die höchste Priorität und wird zuerst behandelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corr, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalBsmtSF mit 1FlrSF sowie GarageCars und GarageArea scheine eine hohe Korrelation zu haben. Dies macht auch Sinn da 1stFlrSF und TotalBsmtSF insofern korrelieren, da die Gesamtanzahl der m² und die des ersten Stockes meist sehr ähnlich wenn nicht sogar gleich sind. Genauso ist es nur logisch und leicht erklärbar, dass umso mehr Platz für Autos in einer Garage sind, auch die Fläche der Garage steigt. Zu hinterfragen wäre lediglich, ob eine Multikolinearität vorliegt und es nicht besser wäre, eine der beiden Variablen zu entfernen. Dazu dann später mehr wenn fehlende Werte untersucht werden und sich vielleicht ohnehin eine Variable dadurch von selbst anbietet.\n",
    "\n",
    "Weitere Pärchen welche korrelieren:\n",
    "\n",
    "* OverallQual scheint auch mit vielen anderen Variablen zu korrelieren. Auch das erscheint plausibel da schon die Bezeichnung \"Overall\" auf eine Bewertung von mehreren Attributen hindeutet. Vor allem mit SalePrice scheint (wenig überraschend) eine hohe Korrelation vorzuliegen.\n",
    "* YearBuilt gemeinsam mit GaragyYrBlt ist auch rational erklärbar.\n",
    "* GrLivArea und TotRmsAbvGrd.\n",
    "\n",
    "Variablen welche mit SalesPrice in deutlichem Zusammenhang stehen und einer nähere Betrachtung Wert sind:\n",
    "\n",
    "* OverallQual\n",
    "* GarageArea / Cars\n",
    "* GrLiveArea\n",
    "* TotalBsmtSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der nächste Schritt ist es zu prüfen, welche Variable des Pärchens GarageArea/GarageCars wir für unser Modell genommen wird. GarageArea gibt vor, wieviele Cars Platz finden und bietet sich somit für eine Beibehaltung an. Trotzdem noch ein paar Checks um sicher zu gehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFeature('GarageArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFeature('GarageCars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soweit keine fehlenden Werte aber natürlich Datensätze welche den Wert 0 in beiden Spalten enthalten. Dies sind Anwesen ohne Garage was soweit Sinn ergibt. Auch die Anzahl ist ident. Daher belassen wir es vorerst bei GarageArea für die weitere Bearbeitung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir unter die Haube der Ziel-Variable SalesPrice blicken, möchte Ich noch einmal die Variable OverallQual etwas näher betrachten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFeature('OverallQual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OverallQual ist eine Aufteilung in 10 Wertungs-Kategorien. Beginnend bei 1 als schlechteste Kategorie, bishin zu 10 als beste. Dabei handelt es sich um int64 Ganzzahlen welche aber besser als kategoriale Variable dargestellt werden sollten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 \n",
    "cols = corr.nlargest(k, 'OverallQual')['OverallQual'].index\n",
    "cm = np.corrcoef(train[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Vergleich dazu, die 10 Variablen welche den größten Einfluss auf SalePrice haben: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(train[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man sieht schon, dass einige Variablen in beiden Diagrammen vorkommen. Diese werden in späterer Folge wohl eine wichtige Rolle spielen, daher nachfolgend eine Auflistung mit kurzer Beschreibung:\n",
    "\n",
    "* YearBuilt - wann das Objekt erbaut wurde\n",
    "* GarageAreas - Platz der Garage in square feet\n",
    "* GrLivingArea - Wohnbereich über dem Grund in square fett\n",
    "* FullBath - Volle Badzimmer über dem Grund. Interessat wäre noch zu wissen, was mit \"Voll\" gemeint ist.\n",
    "* TotalBsmtSF - Summe der square feet des Untergeschosses\n",
    "* OverallQual - Gesamtzustand des Objekts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse der Feature-Variablen\n",
    "Kommen wir zurück zu unseren Feature-Variablen. Diese werden wir in den nächsten Schritten einzeln unter die Lupe nehmen und gegebenfalls anpassungen vornehmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YearBuilt\n",
    "Wann das Objekt erbaut wurde. Erwartet werden Ganzzahlen im üblichen Jahresformat, keine negativen und keine fehlenden Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFeature('YearBuilt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es war zu erwarten, dass wir hier keine Normalverteilung vorfinden werden. Derzeit ist die Variable noch als numersicher Wert im Datenset. Eine Umwandlung durch OneHotEnconding ist je nach eingesetztem Algorithmus abzuwägen. Auch eine Einteilung in Kategorien wie zB \"vor 1900\", \"1900-1930\", \"vor 1.WK\" etc. könnte eine nähere Untersuchung wert sein. Vorerst ist wichtig, dass wir keine negativen oder fehlenden Werte haben. \n",
    "\n",
    "In Verbindung mit SalePrice ist eine schwache logarithmische Kurve (Anstieg) zu erkennen. Interessant wäre zu wissen, ob die Verkaufspreise der Vergangenheit Inflationsbereinigt wurden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GarageArea\n",
    "Platz der Garage in square feet. Erwartet werden float64 Werte, keine negativen und keine fehlenden Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFeature('GarageArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printSkewKurt('GarageArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Verteilung geht schon in Richtung Normalverteilung könnte aber vielleicht durch eine log-Transformierung verbessert werden. Dies prüfen wir im nächsten Schritt. Natürlich gibt es die \"Ausreißer\" mit 0ft² von Objekten welche keine Garage besitzen. Um diese müssen wir uns zuvor kümmer da wir 0 Werte nicht log-Transformieren können. \n",
    "\n",
    "Auffallend die die 4 Ausreißer rechts unten welche auf sehr große Garagen aber trotzdem einen niedrigen Verkaufspreis darstellen. Werfen wir einen genaueren Blick auf diese 4 Objekte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.nlargest(4, columns=['GarageArea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei diesen 4 Objekten konnten keine Auffälligkeiten gefunden werden. Belassen wir sie im Datensatz. Die Beziehung zwischen GarageArea und SalePrice ist wenn dann nur schwach linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die log-Transformierung durchzuführen fügen wir dem Datenset die Spalte \"hasGarage\" hinzu welchen einen Binärwert halten wird. 0=keine Garage 1=hat Garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['HasGarage'] = pd.Series(len(train['GarageArea']), index=train.index)\n",
    "train['HasGarage'] = 0 \n",
    "train.loc[train['GarageArea']>0,'HasGarage'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir die log-Transformierung nur auf die Zeilen anwenden, welche in \"hasGarage\" 1 haben. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.loc[train['HasGarage']==1,'GarageArea'] = np.log(train['GarageArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train[train['GarageArea']>0]['GarageArea'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train[train['GarageArea']>0]['GarageArea'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % train[train['GarageArea']>0]['GarageArea'].skew())\n",
    "print(\"Kurtosis: %f\" % train[train['GarageArea']>0]['GarageArea'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es scheint als hätten wir uns ein wenig von der Normalverteilung wegbewegt weshalb wird von einer log-Transformierung von \"GarageArea\" absehen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GrLivArea\n",
    "Wohnbereich über dem Grund in square feet. Erwartet werden float64 Werte, keine negativen und keine fehlenden Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFeature('GrLivArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printSkewKurt('GrLivArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verteilung scheint in Ordnung und es liegt eine lineare Beziehung vor. Dies war auch so zu erwarten dass mit steigender Größe des Wohnbereichs auch der Verkaufspreis ansteigt.\n",
    "\n",
    "Trotzdem starten wir auch hier den Versuch einer log-Transformation und sehen wie sich die Verteilung ändert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['GrLivArea'] = np.log(train['GrLivArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFeature('GrLivArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printSkewKurt('GrLivArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sieht deutlich besser aus weshalb wir die log-Transformierung beibehalten werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FullBath\n",
    "\n",
    "Volle Badzimmer über dem Grund. Erwartet werden Ganzzahlen, keine negativen und keine fehlenden Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFeature('FullBath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen, es gibt 4 Kategorien von Bädern. 0-4 Stück pro Objekt. Eine Umwandlung in kategorische Variablen ist nicht wünschenswert da wir die Wertung beibehalten wollen. Mehr Badezimmer sind nunmal mehr wert als weniger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TotalBsmtSF\n",
    "Summe der square feet des Kellers. Erwartet werden float Werte, keine negativen und keine fehlenden Werte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFeature('TotalBsmtSF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier sieht es nach einer linearen Beziehung aus bei der eine schöne Verteilung vorliegt. Die einzige Sache welche zu bedenken ist, wie gehen wir mit den 0 Werten (kein Keller) um. Wir wenden die selbe Vorgehensweise als bei \"GarageArea\" an da die Ausgangssituation nahezu ident ist und prüfen, ob eine log-Transformation Sinn macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printSkewKurt('TotalBsmtSF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['HasBasement'] = pd.Series(len(train['TotalBsmtSF']), index=train.index)\n",
    "train['HasBasement'] = 0 \n",
    "train.loc[train['TotalBsmtSF']>0,'HasBasement'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['HasBasement']==1,'TotalBsmtSF'] = np.log(train['TotalBsmtSF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train[train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train[train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % train[train['TotalBsmtSF']>0]['TotalBsmtSF'].skew())\n",
    "print(\"Kurtosis: %f\" % train[train['TotalBsmtSF']>0]['TotalBsmtSF'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier sieht es nach einer Verbesserung im Sinne von einer Annäherung an eine Normalverteilung aus weshalb die log-Transformierung beibehalten wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OverallQual\n",
    "Gesamtzustand des Objekts. 10 Kategorien von 1-10 wobei 1 die schlechteste und 10 die beste Bewertung darstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkFeature('OverallQual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusammenfassung der Analyse der Feature-Variablen\n",
    "Nach Begutachtung der vielversprechendsten Feature-Variablen wird das Trainings-Datenset wie folgt aufgebaut:\n",
    "\n",
    "* YearBuilt - wird nicht geändert\n",
    "* GarageAreas - wird nicht geändert (GarageCars wird weggelassen)\n",
    "* GrLivingArea - log-Transformierung\n",
    "* FullBath - wird nicht geändert\n",
    "* TotalBsmtSF - log-Transformierung\n",
    "* OverallQual - wird nicht geändert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ziel-Variable SalesPrice\n",
    "Als nächste wird unsere Ziel-Variable etwas genauer unter die Lupe genommen. Die Erwartung ist, dass es sich um einen numerischen Wert handelt welcher angibt, um welchen USD Betrag die Immobilie verkauft wurde. Da es sich um einen Verkaufspreis handelt, nehme ich im Vorfeld an, dass es sich um keine Normalverteilung handeln wird und wir eine log-Transformierung vornehmen werden müssen. Negative Werte werden ebenfalls nicht erwartet. Lassen wir die Zahlen sprechen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkFeature('SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie erwartet bestätigen sich die beiden Annahmen, dass es sich um keine Normalverteilung handelt und keine negativen Werte vorhanden sind. Mittels Scatterplot-Matrix betrachten wir die Zusammenhänge unserer Ziel-Variable mit den bereits weiter oben gefundenen vielversprechenden Feature-Variablen\n",
    "* YearBuilt - wann das Objekt erbaut wurde\n",
    "* GarageArea - Platz der Garage in square feet\n",
    "* GrLivingArea - Wohnbereich über dem Grund in square fett\n",
    "* FullBath - Volle Badzimmer über dem Grund. Interessat wäre noch zu wissen, was mit \"Voll\" gemeint ist.\n",
    "* TotalBsmtSF - Summe der square feet des Keller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'TotalBsmtSF', 'FullBath', 'YearBuilt', 'GarageArea']\n",
    "sns.pairplot(train[cols], size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Scatter-Plots zeigen keine Überraschungen. SalesPrice zeigt 4 Punkte welche herausstechen und möglicherweise Outlier sind. Bei TotalBsmtSF ist vor allem ein Punkt sehr markant weit außen und einer näheren Betrachtung wert. Wir wollen aber zuerst noch SalesPrice weiter analysieren und beginnen mit deren Skalierung und log-Transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['SalePrice'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler_y = RobustScaler()\n",
    "robust_scaler_y.fit(y)\n",
    "salePrice_scaled_robust = robust_scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax0, ax1) = plt.subplots(1, 2)\n",
    "\n",
    "ax0.hist(train['SalePrice'].values, bins=100)\n",
    "ax0.set_ylabel('Price')\n",
    "ax0.set_xlabel('Target')\n",
    "ax0.set_title('SalePrice distribution')\n",
    "\n",
    "ax1.hist(salePrice_scaled_robust, bins=100)\n",
    "ax1.set_ylabel('Price')\n",
    "ax1.set_xlabel('Target')\n",
    "ax1.set_title('SalePrice-scaled distribution')\n",
    "\n",
    "f.suptitle(\"Vergleich unscaled und RobustScaler\", y=0.035)\n",
    "f.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeit die log-Transformation anzuwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['SalePrice'] = np.log(train['SalePrice'])\n",
    "checkFeature('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Skalierung und der log-Transformation ist jetzt eine besser Verteilung der Ziel-Variable gegeben. Der Probability-Plot (eine Spezialform des Q-Q Plots) zeigt uns, dass die Daten nahe an der roten Linie liegen was auf eine Normalverteilung deutet. Dies wird durch das darüber befindliche Histogramm ebenfalls bestätigt wodurch unsere Ziel-Variable soweit für die Modellierung vorbereitet ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau Test und Train Datenset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['YearBuilt','GarageArea','OverallQual', 'GrLivArea', 'TotalBsmtSF', 'FullBath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reduced = pd.read_csv('test.csv')[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reduced.fillna(test_reduced.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YearBuilt      0\n",
       "GarageArea     0\n",
       "OverallQual    0\n",
       "GrLivArea      0\n",
       "TotalBsmtSF    0\n",
       "FullBath       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reduced.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "test_reduced['HasBasement'] = pd.Series(len(test_reduced['TotalBsmtSF']), index=test_reduced.index)\n",
    "test_reduced['HasBasement'] = 0\n",
    "test_reduced.loc[test_reduced['TotalBsmtSF']>0,'HasBasement'] = 1\n",
    "test_reduced.loc[test_reduced['HasBasement']==1,'TotalBsmtSF'] = np.log(test_reduced['TotalBsmtSF'])\n",
    "test_reduced['GrLivArea'] = np.log(test_reduced['GrLivArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reduced.drop('HasBasement', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>FullBath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1961</td>\n",
       "      <td>730.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.797940</td>\n",
       "      <td>6.782192</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1958</td>\n",
       "      <td>312.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.192182</td>\n",
       "      <td>7.192182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997</td>\n",
       "      <td>482.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.395722</td>\n",
       "      <td>6.833032</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YearBuilt  GarageArea  OverallQual  GrLivArea  TotalBsmtSF  FullBath\n",
       "0       1961       730.0            5   6.797940     6.782192         1\n",
       "1       1958       312.0            6   7.192182     7.192182         1\n",
       "2       1997       482.0            5   7.395722     6.833032         2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reduced.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YearBuilt',\n",
       " 'GarageArea',\n",
       " 'OverallQual',\n",
       " 'GrLivArea',\n",
       " 'TotalBsmtSF',\n",
       " 'FullBath',\n",
       " 'SalePrice']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.append('SalePrice')\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reduced = pd.read_csv('train.csv')[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "train_reduced['HasBasement'] = pd.Series(len(train_reduced['TotalBsmtSF']), index=train_reduced.index)\n",
    "train_reduced['HasBasement'] = 0 \n",
    "train_reduced.loc[train_reduced['TotalBsmtSF']>0,'HasBasement'] = 1\n",
    "train_reduced.loc[train_reduced['HasBasement']==1,'TotalBsmtSF'] = np.log(train_reduced['TotalBsmtSF'])\n",
    "train_reduced['GrLivArea'] = np.log(train_reduced['GrLivArea'])\n",
    "train_reduced['SalePrice'] = np.log(train_reduced['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reduced.drop('HasBasement', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003</td>\n",
       "      <td>548</td>\n",
       "      <td>7</td>\n",
       "      <td>7.444249</td>\n",
       "      <td>6.752270</td>\n",
       "      <td>2</td>\n",
       "      <td>12.247694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1976</td>\n",
       "      <td>460</td>\n",
       "      <td>6</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>7.140453</td>\n",
       "      <td>2</td>\n",
       "      <td>12.109011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>608</td>\n",
       "      <td>7</td>\n",
       "      <td>7.487734</td>\n",
       "      <td>6.824374</td>\n",
       "      <td>2</td>\n",
       "      <td>12.317167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YearBuilt  GarageArea  OverallQual  GrLivArea  TotalBsmtSF  FullBath  \\\n",
       "0       2003         548            7   7.444249     6.752270         2   \n",
       "1       1976         460            6   7.140453     7.140453         2   \n",
       "2       2001         608            7   7.487734     6.824374         2   \n",
       "\n",
       "   SalePrice  \n",
       "0  12.247694  \n",
       "1  12.109011  \n",
       "2  12.317167  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduced.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YearBuilt      0\n",
       "GarageArea     0\n",
       "OverallQual    0\n",
       "GrLivArea      0\n",
       "TotalBsmtSF    0\n",
       "FullBath       0\n",
       "SalePrice      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduced.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prädiktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.remove('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_reduced['SalePrice']\n",
    "X = train_reduced[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lineares Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8163983013880629"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einfach ohne Parameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9683352873474684"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_regressor = RandomForestRegressor(n_estimators=10)\n",
    "forest_regressor.fit(X, y.ravel())\n",
    "forest_regressor.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([131039.47827211, 139129.46229033, 150250.09426707, ...,\n",
       "       151697.64245361, 100187.96538208, 231177.9386026 ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_pred_y = forest_regressor.predict(test_reduced)\n",
    "forest_pred_y = np.exp(forest_pred_y)\n",
    "forest_pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning mittels K-Fold und Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   41.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  9.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_tuned = RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf_tuned, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X, y.ravel())\n",
    "\n",
    "## WICHTIG: Laufzeit mit 5Kfold ca 10min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 400,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 30,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned.fit(X, y.ravel())\n",
    "rf_tuned.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([129775.90385113, 148534.08092256, 154451.92017118, ...,\n",
       "       144064.0545659 , 111011.43176321, 232733.85103649])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_tuned_pred_y = rf_tuned.predict(test_reduced)\n",
    "rf_tuned_pred_y = np.exp(rf_tuned_pred_y)\n",
    "rf_tuned_pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission File erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': rf_tuned_pred_y})\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    \n",
    "target = 'SalePrice'\n",
    "\n",
    "def checkFeature(feature):\n",
    "    checkNAs(feature)\n",
    "    checkForNegatives(feature)\n",
    "    overview(feature)\n",
    "    plotDistribution(feature)\n",
    "    if feature != target:\n",
    "        plotRelationToTarget(feature)\n",
    "    \n",
    "def checkNAs(feature):\n",
    "    if train[feature].isna().sum() > 0:\n",
    "        print(bcolors.FAIL + \"Sum NAs: \" + str(train[feature].isna().sum()))\n",
    "    else:\n",
    "        print(bcolors.OKGREEN + \"No NAs\" +bcolors.ENDC)\n",
    "\n",
    "def checkForNegatives(feature):\n",
    "    if any(train[feature]<0):\n",
    "        print (bcolors.WARNING + \"Warning feature has negative value!\" + bcolors.ENDC)\n",
    "    else:\n",
    "        print (bcolors.OKGREEN + \"No negative values\" + bcolors.ENDC)\n",
    "\n",
    "def plotDistribution(feature):\n",
    "    sns.distplot(train[feature], fit=norm);\n",
    "    fig = plt.figure()\n",
    "    res = stats.probplot(train[feature], plot=plt)\n",
    "    \n",
    "def plotRelationToTarget(feature):\n",
    "    data_temp = pd.concat([train[target], train[feature]], axis=1)\n",
    "    data_temp.plot.scatter(x=feature, y=target, ylim=(0,800000));\n",
    "        \n",
    "def overview(feature):\n",
    "    print(train[feature].describe())\n",
    "    print(bcolors.HEADER + \"Head\" +bcolors.ENDC)\n",
    "    print(train[feature].head(3))\n",
    "    \n",
    "def printSkewKurt(feature):\n",
    "    print(\"Skewness: %f\" % train[feature].skew())\n",
    "    print(\"Kurtosis: %f\" % train[feature].kurt())\n",
    "    \n",
    "def calculate_performance(prediction, actual, scaler):\n",
    "    if scaler == True:\n",
    "        p = scaler.inverse_transform(prediction.reshape(-1,1))\n",
    "        a = scaler.inverse_transform(actual.reshape(-1,1))\n",
    "    else:\n",
    "        p = prediction\n",
    "        a = actual\n",
    "        \n",
    "    mse = mean_squared_error(a, p)\n",
    "    err = np.sqrt(mse)\n",
    "    r2 = r2_score(a, p)\n",
    "    mae = median_absolute_error(a, p)\n",
    "    \n",
    "    return (mse, err, r2, mae)\n",
    "\n",
    "def print_performance(measure_tuple):\n",
    "    \n",
    "    mse = measure_tuple[0]\n",
    "    err = measure_tuple[1]\n",
    "    r2 = measure_tuple[2]\n",
    "    mae = measure_tuple[3]\n",
    "    \n",
    "    print(\"Mean squared error is {}\".format(str(mse)))\n",
    "    print(\"Positive mean error is {}\".format(str(err)))\n",
    "    print(\"Overall R² is {}\".format(str(r2)))\n",
    "    print(\"Median absolute error is {}\".format(str(mae)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 233.85,
   "position": {
    "height": "40px",
    "left": "448px",
    "right": "20px",
    "top": "120px",
    "width": "471px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
